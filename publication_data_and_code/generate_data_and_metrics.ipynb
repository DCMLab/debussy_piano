{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load\n",
    "\n",
    "Before you can run this notebook, make sure you have Python 3.10 installed and execute `python3 -m pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import wavescapes\n",
    "from wavescapes.color import circular_hue\n",
    "\n",
    "import etl\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the notebook to produce the defaults from the paper. For more information on available normalization methods see the section \"Loading magnitude-phase matrices\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DEBUSSY_REPO = '..'\n",
    "DATA_FOLDER = '.'\n",
    "DEFAULT_FIGURE_SIZE = 2286\n",
    "EXAMPLE_FNAME = 'l123-08_preludes_ondine'\n",
    "how = '0c'\n",
    "indulge = True\n",
    "norm_method = (how, indulge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading metadata\n",
    "Metadata for all pieces contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metadata = etl.get_metadata(DEBUSSY_REPO)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `year` contains composition years as the middle between beginning and end  of the composition span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metadata.year.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series `median_recording` contains median recording times in seconds, retrieved from the Spotify API. the Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metadata.median_recording.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns mirroring a piece's activity are currently:\n",
    "* `qb_per_minute`: the pieces' lengths (expressed as 'qb' = quarterbeats) normalized by the median recording times; a proxy for the tempo\n",
    "* `sounding_notes_per_minute`: the summed length of all notes normalized by the piece's duration (in minutes)\n",
    "* `sounding_notes_per_qb`: the summed length of all notes normalized by the piece's length (in qb)\n",
    "Other measures of activity could be, for example, 'onsets per beat/second' or 'distinct pitch classes per beat/second'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pitch Class Vectors (PCVs)\n",
    "An `{fname -> pd.DataFrame}` dictionary where each `(NX12)` DataFrame contains the absolute durations (expressed in quarter nots) of the 12 chromatic pitch classes for the `N` slices of length = 1 quarter note that make up the piece `fname`. The IntervalIndex reflects each slice's position in the piece. Set `pandas` to False to retrieve NumPy arrays without the IntervalIndex and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pcvs = etl.get_pcvs(DEBUSSY_REPO, pandas=True)\n",
    "etl.test_dict_keys(pcvs, metadata)\n",
    "pcvs[EXAMPLE_FNAME].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wavescapes` library allows for creating a wavescape directly from a PCV matrix. Here es one showing the second coefficient: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "coeff = 2\n",
    "label = etl.make_wavescape_label(EXAMPLE_FNAME, how, indulge, coeff=coeff)\n",
    "path = os.path.join('figures', etl.make_filename(EXAMPLE_FNAME, how, indulge, coeff=coeff, ext='.png'))\n",
    "wavescapes.single_wavescape_from_pcvs(pcvs[EXAMPLE_FNAME],\n",
    "                                      width=DEFAULT_FIGURE_SIZE,\n",
    "                                      coefficient=coeff,\n",
    "                                      save_label=path,\n",
    "                                      magn_stra=how,\n",
    "                                      output_rgba=True,\n",
    "                                      aw_per_tick=10,\n",
    "                                      tick_factor=10,\n",
    "                                      label=label,\n",
    "                                      label_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pitch Class Matrices\n",
    "An `{fname -> np.array}` dictionary where each `(NxNx12)` array contains the aggregated PCVs for all segments that make up a piece. The square matrices contain values only in the upper right triangle, with the lower left beneath the diagonal is filled with zeros. The values are arranged such that row 0 correponds to the original PCV, row 1 the aggregated PCVs for all segments of length = 2 quarter notes, etc. For getting the segment reaching from slice 3 to 5 (including), i.e. length 3, the coordinates are `(2, 5)` (think x = 'length - 1' and y = index of the last slice included).\n",
    "\n",
    "The following example shows the upper left 3x3 submatrix where\n",
    "* the first three entries (which are PCVs of size 12) correspond to the pitch class distributions of the piece's first three quarternote slices,\n",
    "* the two last vectors of the second row each correspond to a sum of two adjacent vectors above, and\n",
    "* the last entry of the the third row corresponds to the sum all three PCVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcms = etl.get_pcms(DEBUSSY_REPO)\n",
    "etl.test_dict_keys(pcms, metadata)\n",
    "print(f\"Shape of the PCM for {EXAMPLE_FNAME}: {pcms[EXAMPLE_FNAME].shape}\")\n",
    "pcms[EXAMPLE_FNAME][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Discrete Fourier Transforms\n",
    "`{fname -> np.array}` containing `(NxNx7)` complex matrices. For instance, here's the first element, a size 7 complex vector with DFT coefficients 0 through 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfts = etl.get_dfts(DEBUSSY_REPO)\n",
    "etl.test_dict_keys(dfts, metadata)\n",
    "print(f\"Shape of the DFT for {EXAMPLE_FNAME}: {dfts[EXAMPLE_FNAME].shape}\")\n",
    "dfts[EXAMPLE_FNAME][0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the 7 complex numbers as magnitude-phase pairs. In the following we use magnitude-phase-matrices of this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_coeff(dfts[EXAMPLE_FNAME], 0, 0, deg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, values can also be inspected as strings where the numbers are rounded and angles are shown in degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_coeff(dfts[EXAMPLE_FNAME], 0, 0, deg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading magnitude-phase matrices\n",
    "`{fname -> np.array}` where each of the `(NxNx6x2)` matrices contains the 6 relevant DFT coefficients converted into magnitude-phase pairs where the magnitudes have undergone at least one normalization, i.e. are all within [0,1]. The first time the notebook runs, the matrices are computed and pickled to disk, from where they can be loaded on later runs.\n",
    "\n",
    "The parameter `norm_params` can be one or several `(how, indulge)` pairs where `indulge` is a boolean and `how ∈ {'0c', 'post_norm', 'max_weighted', 'max'}`.\n",
    "\n",
    "### Normalizing magnitudes\n",
    "\n",
    "The available normalization methods for `how` are:\n",
    "* **'0c'** default normalisation, will normalise each magnitude by the 0th coefficient (which corresponds to the sum of the weight of each pitch class). This ensures onlypitch class distribution whose periodicity exactly match the coefficient's periodicity can reach the value of 1.\n",
    "* **'post_norm'** based on the 0c normalisation but \"boost\" the space of all normalized magnitude so the maximum magnitude observable is set to the max opacity value. This means that if any PCV in the utm given as input reaches the 0c normalized magnitude of 1, this parameter acts like the '0c' one. This magn_strat should be used with audio input mainly, as seldom PCV derived from audio data can reach the maximal value of normalized magnitude for any coefficient.\n",
    "* **'max'** set the grayscal value 1 to the maximum possible magnitude in the wavescape, and interpolate linearly all other values of magnitude based on that maximum value set to 1. Warning: will bias the visual representation in a way that the top of the visualisation will display much more magnitude than lower levels.\n",
    "* **'max_weighted'** same principle as max, except the maximum magnitude is now taken at the hierarchical level, in other words, each level will have a different opacity mapping, with the value 1 set to the maximum magnitude t this level. This normalisation is an attempt to remove the bias toward higher hierarchical level that is introduced by the 'max' magnitude process cited previously.\n",
    "\n",
    "`indulge` is an additional normalization that we apply to the magnitude based on the phase. Since magnitudes of 1 are possible only for a prototypical phase sitting on the unit circle, you can set this parameter to True to normalize the magnitudes by the maximally achievable magnitude given the phase which is bounded by straight lines between adjacent prototypes. (Musical prototypes are visualized in the [midiVERTO webApp](https://dcmlab.github.io/midiVERTO/#/analysis)) The pitch class vectors that benefit most from this normalization in terms of magnitude gain are those whose phase is exactly between two prototypes, such as the \"octatonic\" combination O₀,₁. The maximal \"boosting\" factors for the first 5 coefficients are `{1: 1.035276, 2: 1.15470, 3: 1.30656, 4: 2.0, 5: 1.035276}`. The sixth coefficient's phase can only be 0 or pi so it remains unchanged. Use this option if you want to compensate for the smaller magnitude space of the middle coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_phase_path = os.path.join(DATA_FOLDER, 'pickled_magnitude_phase_matrices')\n",
    "mag_phase_mx_dict = etl.get_magnitude_phase_matrices(dfts=dfts, data_folder=mag_phase_path, norm_params=norm_method)\n",
    "etl.test_dict_keys(mag_phase_mx_dict, metadata)\n",
    "print(f\"Shape of the magnitude-phase matrix for {EXAMPLE_FNAME}: {mag_phase_mx_dict[EXAMPLE_FNAME].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary wavescapes\n",
    "\n",
    "This cell depends on the previously loaded magnitude-phase matrices, i.e. a conscious choice of a normalization method has been made above.\n",
    "\n",
    "`get_most_resonant` returns three `{fname -> nd.array}` dictionaries where for each piece, the three `(NxN)` matrices correspond to\n",
    "\n",
    "1. the index between 0 and 5 of the most resonant of the six DFT coefficient 1 through 6\n",
    "2. its magnitude\n",
    "3. the inverse entropy of the 6 magnitudes\n",
    "\n",
    "The following example shows these 3 values for the bottom row of the example summary wavescape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "max_coeffs, max_mags, inv_entropies = etl.get_most_resonant(mag_phase_mx_dict)\n",
    "np.column_stack((max_coeffs[EXAMPLE_FNAME][:3],\n",
    "max_mags[EXAMPLE_FNAME][:3],\n",
    "inv_entropies[EXAMPLE_FNAME][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot these values in a wavescape, they need to be transformed into color values. The function `most_resonant2color()` attributes six equidistant hues to the most resonant coefficients and takes one of the other two matrices to adapt the opacity.\n",
    "\n",
    "In the first example the opacity shows the magnitude of the most resonant coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = utils.most_resonant2color(max_coeffs[EXAMPLE_FNAME], max_mags[EXAMPLE_FNAME])\n",
    "by_entropy = False\n",
    "label = etl.make_wavescape_label(EXAMPLE_FNAME, how, indulge, by_entropy=by_entropy)\n",
    "ws = wavescapes.Wavescape(colors, width=DEFAULT_FIGURE_SIZE)\n",
    "ws.draw(aw_per_tick=10, tick_factor=10, label=label, label_size=20)\n",
    "path = os.path.join('figures', etl.make_filename(EXAMPLE_FNAME, how, indulge, summary_by_entropy=by_entropy, ext='.png'))\n",
    "plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second example, the opacity corresponds to the inverse normalized entropy of the 6 magnitudes. In other words, opacity is maximal if the most resonant coefficient is the only one with magnitude > 0; whereas the color is white when all coefficients have the same magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = utils.most_resonant2color(max_coeffs[EXAMPLE_FNAME], inv_entropies[EXAMPLE_FNAME])\n",
    "by_entropy = True\n",
    "label = etl.make_wavescape_label(EXAMPLE_FNAME, how, indulge, by_entropy=by_entropy)\n",
    "ws = wavescapes.Wavescape(colors, width=DEFAULT_FIGURE_SIZE)\n",
    "ws.draw(aw_per_tick=10, tick_factor=10, label=label, label_size=20)\n",
    "path = os.path.join('figures', etl.make_filename(EXAMPLE_FNAME, how, indulge, summary_by_entropy=by_entropy, ext='.png'))\n",
    "plt.savefig(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "In this section, we compile a dataframe containing all metrics results, both melted and not. The metrics will then be stored and used for testing in RFILE. Optional plots and tests can be done by adjusting the parameters of the wrapper function `get_metric` that can be found in `etl.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start from the available metadata for each piece\n",
    "metadata_metrics = metadata.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevalence of each coefficient\n",
    "\n",
    "Prevalence of coefficient $n$ in a piece: $W(n)=1/N \\sum_{i \\in S(n)} i$ where $N$ is the total number of nodes in the wavescape, $S(n)$ is the set of the indices of the nodes in the summary wavescapes that are attributed to coefficient $n$ (i.e., where coefficient $n$ is the most prominent among the six)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# defining column names\n",
    "cols = [f\"percentage_resonances_{i}\" for i in range(1,7)]\n",
    "\n",
    "metadata_metrics = etl.get_metric('percentage_resonance', metadata_metrics, \n",
    "                              max_coeffs=max_coeffs, cols=cols, \n",
    "                              store_matrix=True, show_plot=True, unified=True,\n",
    "                              save_name='percentage_resonance', title='Percentage Resonance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_metrics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to account for the certainty that a certain coefficient is actually the most resonance, we weigh the previous metric by entropy as follows: $W(n)=1/N \\sum_{i \\in S(n)} w_i$ where $N$ is the total number of nodes in the wavescape, $S(n)$ is the set of the indices of the nodes in the summary wavescapes that are attributed to coefficient $n$ (i.e., where coefficient $n$ is the most prominent among the six), and $w_i$ is the weight (opacity) of the $i$-th node in the summary wavescape, in this case, the entropy of $i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = [f\"percentage_resonances_entropy_{i}\" for i in range(1,7)]\n",
    "\n",
    "metadata_metrics = etl.get_metric('percentage_resonance_entropy', metadata_metrics, \n",
    "                              cols=cols,\n",
    "                              max_coeffs=max_coeffs, inv_entropies=inv_entropies,\n",
    "                              store_matrix=True, show_plot=True, unified=True,\n",
    "                              save_name='percentage_resonance_entropy', title='Percentage Resonance (entropy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_metrics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc analysis of hierarchical prevalence\n",
    "## Moment of Inertia\n",
    "\n",
    "Moment of inertia of coefficient $n$ in the summary wavescape: $I(n)=1/N \\sum_{i \\in S(n)} w_i y_i^2$, where N is the total number of nodes in the wavescape, $S(n)$ is the set of the indices of the nodes in the summary wavescapes that are attributed to coefficient $n$ (i.e., where coefficient n is the most prominent among the six), $w_i$ is the weight (opacity) of the $i$-th node in the summary wavescape, and $y_i$ is the vertical coordinate of the $i$-th node in the summary wavescape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"moments_of_inertia_{i}\" for i in range(1,7)]\n",
    "\n",
    "metadata_metrics = etl.get_metric('moment_of_inertia', metadata_metrics, \n",
    "                              cols=cols,\n",
    "                              max_coeffs=max_coeffs, max_mags=max_mags,\n",
    "                              store_matrix=True, show_plot=True, unified=True,\n",
    "                              save_name='moments_of_inertia', title='Moments of Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_metrics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Theoretic Entropy\n",
    "\n",
    "Measure-theoretic entropy: Let $A={A_1,...,A_k}$ be a (finite) partition of a probability space $(X,P(X),)$: the entropy of the partition $A$ is defined as $H(A)= - \\sum_{i} \\mu(A_i) \\log \\mu(A_i)$. We can take $X$ as the support of the wavescape, $A$ as the set of the connected regions in the unified wavescape, and $\\mu(Y)=(area-of-Y)/(area-of-X)$ for any subset $Y$ of the wavescape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = 'partition_entropy'\n",
    "\n",
    "metadata_metrics = etl.get_metric('partition_entropy', metadata_metrics, \n",
    "                              cols=cols,\n",
    "                              max_coeffs=max_coeffs,\n",
    "                              store_matrix=True, scatter=True, show_plot=True, unified=True, \n",
    "                              save_name='partition_entropy', title='Partition Entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_metrics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decreasing magnitude in height\n",
    "\n",
    "The inverse coherence is the slope of the regression line that starts from the magnitude resonance in the summary wavescape at bottom of the wavescape and reaches the one at the top of the wavescape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'inverse_coherence'\n",
    "metadata_metrics = etl.get_metric('inverse_coherence', metadata_metrics, \n",
    "                              cols=cols,\n",
    "                              max_mags=max_mags,\n",
    "                              store_matrix=True, show_plot=True, unified=True, scatter=True,\n",
    "                              save_name='inverse_coherence', title='Inverse Coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_metrics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the final metrics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "metadata_metrics.reset_index().to_csv('results/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# melting the results to be used for testing on coefficient specific metrics\n",
    "\n",
    "cols = [col for col in metadata_metrics.columns if col[-1].isnumeric()]\n",
    "metadata_metrics = metadata_metrics.reset_index()\n",
    "metadata_melted = pd.melt(metadata_metrics, id_vars=['fname', 'length_qb', 'year', 'last_mc'], value_vars=cols)\n",
    "\n",
    "metadata_melted.reset_index().to_csv('results/results_melted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e75dc31b3b5f9bc495c1e31c21f3223286c169dd95cd54077d02d6f843e27785"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
